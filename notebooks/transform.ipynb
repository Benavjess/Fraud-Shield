{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f485cae",
   "metadata": {},
   "source": [
    "# Data Transform\n",
    "\n",
    "In this notebook, we will ask you a series of questions to evaluate your findings from your EDA. Based on your response & justification, we will ask you to also apply a subsequent data transformation. \n",
    "\n",
    "If you state that you will not apply any data transformations for this step, you must **justify** as to why your dataset/machine-learning does not require the mentioned data preprocessing step.\n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient feature engineering step in this project we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "You will write out this transformed dataframe as a `.csv` file to your `data/` folder.\n",
    "\n",
    "**Note**: Again, note that this dataset is quite large. If you find that some data operations take too long to complete on your machine, simply use the `sample()` method to transform a subset of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a38922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360ca62",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Does your model contain any missing values or \"non-predictive\" columns? If so, which adjustments should you take to ensure that your model has good predictive capabilities? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Drop nameOrig and nameDest from the dataset because they are non-predictive in their raw form and might be overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cfe64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample shape: (50000, 8)\n",
      "Missing values:\n",
      " type              0\n",
      "amount            0\n",
      "oldbalanceOrg     0\n",
      "newbalanceOrig    0\n",
      "oldbalanceDest    0\n",
      "newbalanceDest    0\n",
      "isFraud           0\n",
      "isFlaggedFraud    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "transactions = pd.read_csv(\"../data/bank_transactions.csv\")\n",
    "\n",
    "# Drop non-predictive\n",
    "transactions_transformed = transactions.drop(['nameOrig', 'nameDest'], axis=1)\n",
    "\n",
    "# Use a sample for faster processing (adjust n as needed)\n",
    "transactions_transformed_sample = transactions_transformed.sample(n=50000, random_state=42)\n",
    "\n",
    "# Verify\n",
    "print(\"Sample shape:\", transactions_transformed_sample.shape)\n",
    "print(\"Missing values:\\n\", transactions_transformed_sample.isnull().sum())\n",
    "\n",
    "# Save the transformed sample\n",
    "transactions_transformed_sample.to_csv(\"../data/transactions_transformed_sample.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301be5ef",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Do certain transaction types consistently differ in amount or fraud likelihood? If so, how might you transform the type column to make this pattern usable by a machine learning model? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Yes, transaction types differ significantly in both amount and fraud. Fraud is highly concentrated in TRANSFER and CASH_OUT types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa820db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after encoding: ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud', 'type_CASH_IN', 'type_CASH_OUT', 'type_DEBIT', 'type_PAYMENT', 'type_TRANSFER']\n"
     ]
    }
   ],
   "source": [
    "# one hot encoder\n",
    "\n",
    "transactions = pd.read_csv(\"../data/transactions_transformed_sample.csv\")\n",
    "\n",
    "# One-hot encode \n",
    "transactions_encoded = pd.get_dummies(transactions, columns=['type'], prefix='type')\n",
    "\n",
    "# Verify \n",
    "print(\"Columns after encoding:\", transactions_encoded.columns.tolist())\n",
    "\n",
    "# Save the new transformed sample\n",
    "transactions_encoded.to_csv(\"../data/transactions_transformed_sample_encoded.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952403f",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "After exploring your data, you may have noticed that fraudulent transactions are rare compared to non-fraudulent ones. What challenges might this pose when training a machine learning model? What strategies could you use to ensure your model learns meaningful patterns from the minority class? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "The model may just predict non-fraud for everything. Also, fraud may not have enough examples for the model to learn meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f1422",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote  wait this might be in model training folder instead\n",
    "\n",
    "\n",
    "#transactions = pd.read_csv(\"../data/transactions_transformed_sample_encoded.csv\")\n",
    "\n",
    "#  1. Split features and target\n",
    "X = transactions.drop('isFraud', axis=1)\n",
    "y = transactions['isFraud']\n",
    "\n",
    "#  2. Split into train/test \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Original train fraud count:\\n\", y_train.value_counts())\n",
    "\n",
    "#  3. Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTE train fraud count:\\n\", y_train_smote.value_counts())\n",
    "\n",
    "# 4. Combine back to DataFrame for inspection\n",
    "train_smote = pd.concat([X_train_smote, y_train_smote], axis=1)\n",
    "\n",
    "#5. Save SMOTE version of training data\n",
    "train_smote.to_csv(\"../data/transactions_transformed_train_SMOTE.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17737e9e",
   "metadata": {},
   "source": [
    "## Bonus (optional)\n",
    "\n",
    "Are there interaction effects between variables (e.g., fraud and high amount and transaction type) that aren't captured directly in the dataset? Would it be helpful to manually engineer any new features that reflect these interactions? Apply your data transformations (if any) in the code-block below.\n",
    "\n",
    "Answer Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f48b7af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81cbfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out newly transformed dataset to your folder\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
